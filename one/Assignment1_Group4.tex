\documentclass{article}
\newcommand{\tab}[1]{\hspace{10 mm}\rlap{#1}}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{xcolor,colortbl}
\usepackage[]{algorithm2e}

%\usepackage{soul}

\begin{document}

\title{Autonomous Agents 1 \\ Assignment 1}

\author{By Group 4: Gieske, Gornishka, Loor, Radscum}
\maketitle

\pagebreak

\section*{Introduction}
This report discusses a predator versus prey Markov Decision Process (MDP). In order to analyze this MDP, it was implemented. This MDP consists of an 11 $\times$ 11 toroidal grid. The predator and the prey are placed on the grid, after which the  predator must catch the prey. Both can move vertically and horizontally across the grid as well as stay put until the next time step. Describing the movements about the grid as North, East, South, West and Wait, the policies of the predator and the prey are as follows:
\begin{center}
	\begin{tabular}{ | l | l | l | l | l | l |}
	\hline
	& North & East & South & West & Wait\\
	\hline
	Predator & 0.2 & 0.2 & 0.2 & 0.2 & 0.2 \\ 
	\hline
	Prey & 0.05 & 0.05 & 0.05 & 0.05 & 0.8 \\
	\hline
	\end{tabular}
\end{center}
The predator and prey move about on the grid as specified but the policy. However, the prey does not move towards the predator. After the prey is caught, the episode ends and the game is reverted to starting positions. Catching the prey gives a reward of 10, 0 otherwise.

This implementation contains an execution of the game, policy evaluation, policy iteration and value iteration. The performance of these functions are analyzed in order to research the behaviour of the agents. The results of these functions are also compared with one another as part of analyzation.

\pagebreak

\section*{Theory}
\subsection*{Iterative policy evaluation}
Iterative policy iteration is used compute the state-value function $v_{\pi}$ for an arbitrary policy $\pi$. It is a stationary algorithm where the goal state and the arbitrary policy are static. In this case, it means that the goal state, the prey, remains on the same location. It uses the following algorithm as described in Barto and Sutton [source]: \\

\begin{algorithm}[H]
Input $\pi$, the policy to be evaluated \\
Initialize an array v(s) = 0, for all s $\in S^{+}$ \\
Repeat \\
\tab{$\Delta \leftarrow 0$} \\
\tab{For each $s \in S$:} \\
\tab{temp $\leftarrow$ v(s)} \\
\tab{v(s) $\leftarrow \; \sum_a \pi(a|s) \sum_{s'} p(s'|s, a)[r(s, a, s') + \gamma v(s')]$} \\
\tab{$\Delta \leftarrow max(\Delta, | temp - v(s) |)$} \\ 
until $\Delta < \theta $(a small positive number)\\
Output v $\approx $ v(s)\\
\end{algorithm}

Where: \\
$\pi(a|s)$ is an action chosen, given the state. \\
$p(s'|s, a)$ is a transition function. \\
$r(s, a, s')$ is a reward function. \\
$\gamma v(s')$ is the discounted state. \\ % is it?

\subsection*{Policy improvement}
Policy improvement is used to find an optimal, deterministic policy. This is, again a stationary function. Again, from Barto and Sutton [source]:\\

\begin{algorithm}[H]
1. Initialization \\
\hspace{4 mm} v(s) $Ã­n \mathcal{R}$ and $\pi(s) \in \mathcal{A}(s)$  arbitrarily for all s $\in$ S \\

2. Policy evaluation \\
\hspace{4 mm} Repeat \\
\tab{$\Delta \leftarrow 0$} \\
\tab{For each $s \in S$:} \\
\tab{temp $\leftarrow$ v(s)} \\
\tab{v(s) $\leftarrow \; \sum_{s'} p(s'|s, \pi(s))[r(s, a, s') + \gamma v(s')]$} \\
\tab{$\Delta \leftarrow max(\Delta, | temp - v(s) |)$} \\ 
\hspace{4 mm} until $\Delta < \theta $(a small positive number)\\

3. Policy improvement \\
\hspace{4 mm} Policy stable $\leftarrow$ true \\
\hspace{4 mm} For each s $\in$ S: \\
\tab{temp $\leftarrow \pi(s)$} \\  
\tab{v(s) $\leftarrow \; arg \max\limits_a \sum_{s'} p(s'|s, a)[r(s, a, s') + \gamma v(s')]$} \\
\hspace{4 mm} if $temp \neq \pi(s)$, policy stable $\leftarrow$ false
\end{algorithm}

\subsection*{Value iteration}

\begin{algorithm}[H]
From Barto and Sutton [source]\\
Repeat \\
$\Delta \leftarrow 0$ \\
For each s $\in$ S: \\
\tab{$temp \leftarrow v(s)$} \\
\tab{$v(s) \leftarrow \max\limits_a \sum_{s'} p(s'|s, a)[r(s, a, s') + \gamma v(s')]$} \\
\tab{$\Delta \leftarrow max(temp, |v(s)|)$} \\
until $\Delta \leftarrow \theta$ (a small positive number)\\

Output a deterministic policy $\pi$, such that \\
$\pi (s) = arg \max\limits_a \sum_{s'} p(s'|s, a)[r(s, a, s') + \gamma v(s')]$
\end{algorithm}

\section*{Implementation}

\noindent The current implementation consists of the following classes:
\begin{description}
	\item[Predator] \hfill \\ 
	This class implements the predator. It contains the policy and actions as described in the introduction.
	
	\item[Prey] \hfill \\ 
	This class implements the prey. It contains the policy and actions as described in the introduction.
	
	\item[Environment] \hfill \\ 
	This class implements the environment. This is a toroidal 11 $\times$ 11 grid. Objects, in this case the prey and predator, can be placed on this environment and move about.
	
	\item[Game] \hfill \\ 
	This class implements the game itself. It is possible to start the game. It also performs policy evaluation, policy iteration and value iteration.
	
\end{description}


\section*{Analysis}


\subsection*{Simulator for the environment}


\begin{center}
	\begin{tabular}{  l ||  l }
		Avarage run time & Standard deviation \\ 
		\hline
		296 rounds &  286.580390118 %avarage run time and standart deviation to be filled here 	
	\end{tabular}
\end{center}

\subsection*{Iterative policy evaluation}
As described in the assignment, policy evaluation must be executed. Policy evaluation is a stationary algorithm which evaluates the agent's policy. The algorithm used is described in Barto and Sutton [source], section 4.1. By analyzing different cases for policy evaluation, the policy of the agent can be analyzed for improvement. It is expected that the policy evaluation values increase around the location of the prey. Therefore, if the agent moves in the direction of the increasing numbers on the grid, it will catch the prey. The locations of the predator are marked green and the locations of the prey are marked red to increase readability. The following cases have been analyzed, using a stationary prey:
\begin{center}
	\begin{tabular}{ | l | l | l |}
	\hline
	Case & Predator & Prey\\
	\hline
	1 & (0,0) & (5,5)\\ 
	\hline
	2 & (2,3) & (5,4)\\ 
	\hline
	3 & (2,10) & (10,0)\\ 
	\hline
	4 & (10,10) & (0,0)\\ 
	\hline
	\end{tabular}
\end{center}

Starting with case 1, the following result is calculated:

\begin{center}
\scalebox{0.7}
	{
	\begin{tabular}{ |l | l | l | l | l | l | l | l | l | l | l | l|}
	\hline
	\multicolumn{12}{|c|}{Value grid in loop 32, Predator(0,0), Prey(5,5)}\\
	\hline
	Indices y\textbackslash x &0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ 

\hline
0 & \cellcolor{green!40}0.003357 & 0.005538 & 0.010435 & 0.018407 & 0.027837 & 0.033106 & 0.027837 & 0.018407 & 0.010435 & 0.005538 & 0.003357 \\
1 & 0.005538 & 0.009923 & 0.020607 & 0.040185 & 0.067041 & 0.085283 & 0.067041 & 0.040185 & 0.020607 & 0.009923 & 0.005538 \\
2 & 0.010435 & 0.020607 & 0.047865 & 0.105160 & 0.198928 & 0.280822 & 0.198928 & 0.105160 & 0.047865 & 0.020607 & 0.010435 \\
3 & 0.018407 & 0.040185 & 0.105160 & 0.265391 & 0.591645 & 0.991487 & 0.591645 & 0.265391 & 0.105160 & 0.040185 & 0.018407 \\
4 & 0.027837 & 0.067041 & 0.198928 & 0.591645 & 1.650667 & 3.741537 & 1.650667 & 0.591645 & 0.198928 & 0.067041 & 0.027837 \\
5 & 0.033106 & 0.085283 & 0.280822 & 0.991487 & 3.741537 & \cellcolor{red!40}2.850622 & 3.741537 & 0.991487 & 0.280822 & 0.085283 & 0.033106 \\
6 & 0.027837 & 0.067041 & 0.198928 & 0.591645 & 1.650667 & 3.741537 & 1.650667 & 0.591645 & 0.198928 & 0.067041 & 0.027837 \\
7 & 0.018407 & 0.040185 & 0.105160 & 0.265391 & 0.591645 & 0.991487 & 0.591645 & 0.265391 & 0.105160 & 0.040185 & 0.018407 \\
8 & 0.010435 & 0.020607 & 0.047865 & 0.105160 & 0.198928 & 0.280822 & 0.198928 & 0.105160 & 0.047865 & 0.020607 & 0.010435 \\
9 & 0.005538 & 0.009923 & 0.020607 & 0.040185 & 0.067041 & 0.085283 & 0.067041 & 0.040185 & 0.020607 & 0.009923 & 0.005538 \\
10 & 0.003357 & 0.005538 & 0.010435 & 0.018407 & 0.027837 & 0.033106 & 0.027837 & 0.018407 & 0.010435 & 0.005538 & 0.003357 \\
\hline
	\end{tabular}
	}
\end{center}

In this case, it can be seen that moving in a diagonal direction gets the agent to the prey fastest. This takes ten steps. It can also be seen that this is the maximum distance between the predator and the prey.

\begin{center}
\scalebox{0.7}
	{
	\begin{tabular}{ |l | l | l | l | l | l | l | l | l | l | l | l|}
	\hline
	\multicolumn{12}{|c|}{Value grid in loop 32, Predator(2,3), Prey(5,4)}\\
	\hline
	Indices y\textbackslash x &0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ 

\hline
0 & 0.005538 & 0.010435 & 0.018407 & 0.027837 & 0.033106 & 0.027837 & 0.018407 & 0.010435 & 0.005538 & 0.003357 & 0.003357 \\
1 & 0.009923 & 0.020607 & 0.040185 & 0.067041 & 0.085283 & 0.067041 & 0.040185 & 0.020607 & 0.009923 & 0.005538 & 0.005538 \\
2 & 0.020607 & 0.047865 & 0.105160 & \cellcolor{green!40}0.198928 & 0.280822 & 0.198928 & 0.105160 & 0.047865 & 0.020607 & 0.010435 & 0.010435 \\
3 & 0.040185 & 0.105160 & 0.265391 & 0.591645 & 0.991487 & 0.591645 & 0.265391 & 0.105160 & 0.040185 & 0.018407 & 0.018407 \\
4 & 0.067041 & 0.198928 & 0.591645 & 1.650667 & 3.741537 & 1.650667 & 0.591645 & 0.198928 & 0.067041 & 0.027837 & 0.027837 \\
5 & 0.085283 & 0.280822 & 0.991487 & 3.741537 & \cellcolor{red!40}2.850622 & 3.741537 & 0.991487 & 0.280822 & 0.085283 & 0.033106 & 0.033106 \\
6 & 0.067041 & 0.198928 & 0.591645 & 1.650667 & 3.741537 & 1.650667 & 0.591645 & 0.198928 & 0.067041 & 0.027837 & 0.027837 \\
7 & 0.040185 & 0.105160 & 0.265391 & 0.591645 & 0.991487 & 0.591645 & 0.265391 & 0.105160 & 0.040185 & 0.018407 & 0.018407 \\
8 & 0.020607 & 0.047865 & 0.105160 & 0.198928 & 0.280822 & 0.198928 & 0.105160 & 0.047865 & 0.020607 & 0.010435 & 0.010435 \\
9 & 0.009923 & 0.020607 & 0.040185 & 0.067041 & 0.085283 & 0.067041 & 0.040185 & 0.020607 & 0.009923 & 0.005538 & 0.005538 \\
10 & 0.005538 & 0.010435 & 0.018407 & 0.027837 & 0.033106 & 0.027837 & 0.018407 & 0.010435 & 0.005538 & 0.003357 & 0.003357 \\
\hline
	\end{tabular}
	}
\end{center}

With the agent starting at (2,3) and the prey located at (5,4), it will take the predator four steps to reach the prey.

\begin{center}
\scalebox{0.7}
	{
	\begin{tabular}{ |l | l | l | l | l | l | l | l | l | l | l | l|}
	\hline
	\multicolumn{12}{|c|}{Value grid in loop 32, Predator(2,10), Prey(10,0)}\\
	\hline
	Indices y\textbackslash x &0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ 

\hline
0 & 3.741537 & 1.650667 & 0.591645 & 0.198928 & 0.067041 & 0.027837 & 0.027837 & 0.067041 & 0.198928 & 0.591645 & 1.650667 \\
1 & 0.991487 & 0.591645 & 0.265391 & 0.105160 & 0.040185 & 0.018407 & 0.018407 & 0.040185 & 0.105160 & 0.265391 & 0.591645 \\
2 & 0.280822 & 0.198928 & 0.105160 & 0.047865 & 0.020607 & 0.010435 & 0.010435 & 0.020607 & 0.047865 & 0.105160 & \cellcolor{green!40}0.198928 \\
3 & 0.085283 & 0.067041 & 0.040185 & 0.020607 & 0.009923 & 0.005538 & 0.005538 & 0.009923 & 0.020607 & 0.040185 & 0.067041 \\
4 & 0.033106 & 0.027837 & 0.018407 & 0.010435 & 0.005538 & 0.003357 & 0.003357 & 0.005538 & 0.010435 & 0.018407 & 0.027837 \\
5 & 0.033106 & 0.027837 & 0.018407 & 0.010435 & 0.005538 & 0.003357 & 0.003357 & 0.005538 & 0.010435 & 0.018407 & 0.027837 \\
6 & 0.085283 & 0.067041 & 0.040185 & 0.020607 & 0.009923 & 0.005538 & 0.005538 & 0.009923 & 0.020607 & 0.040185 & 0.067041 \\
7 & 0.280822 & 0.198928 & 0.105160 & 0.047865 & 0.020607 & 0.010435 & 0.010435 & 0.020607 & 0.047865 & 0.105160 & 0.198928 \\
8 & 0.991487 & 0.591645 & 0.265391 & 0.105160 & 0.040185 & 0.018407 & 0.018407 & 0.040185 & 0.105160 & 0.265391 & 0.591645 \\
9 & 3.741537 & 1.650667 & 0.591645 & 0.198928 & 0.067041 & 0.027837 & 0.027837 & 0.067041 & 0.198928 & 0.591645 & 1.650667 \\
10 & \cellcolor{red!40}2.850622 & 3.741537 & 0.991487 & 0.280822 & 0.085283 & 0.033106 & 0.033106 & 0.085283 & 0.280822 & 0.991487 & 3.741537 \\

\hline
	\end{tabular}
	}
\end{center}

With the agent starting at (2,10) and the prey at (10,0), it will take the agent four steps to reach the prey. This means that the distance between the predator and the prey is the same as in the previous case. The fact that the grid is toroidal makes this so.

\begin{center}
\scalebox{0.7}
	{
	\begin{tabular}{ |l | l | l | l | l | l | l | l | l | l | l | l|}
	\hline
	\multicolumn{12}{|c|}{Value grid in loop 32, Predator(10,10), Prey(0,0)}\\
	\hline
	Indices y\textbackslash x &0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ 

\hline
0 & \cellcolor{red!40} 2.850622 & 3.741537 & 0.991487 & 0.280822 & 0.085283 & 0.033106 & 0.033106 & 0.085283 & 0.280822 & 0.991487 & 3.741537 \\
1 & 3.741537 & 1.650667 & 0.591645 & 0.198928 & 0.067041 & 0.027837 & 0.027837 & 0.067041 & 0.198928 & 0.591645 & 1.650667 \\
2 & 0.991487 & 0.591645 & 0.265391 & 0.105160 & 0.040185 & 0.018407 & 0.018407 & 0.040185 & 0.105160 & 0.265391 & 0.591645 \\
3 & 0.280822 & 0.198928 & 0.105160 & 0.047865 & 0.020607 & 0.010435 & 0.010435 & 0.020607 & 0.047865 & 0.105160 & 0.198928 \\
4 & 0.085283 & 0.067041 & 0.040185 & 0.020607 & 0.009923 & 0.005538 & 0.005538 & 0.009923 & 0.020607 & 0.040185 & 0.067041 \\
5 & 0.033106 & 0.027837 & 0.018407 & 0.010435 & 0.005538 & 0.003357 & 0.003357 & 0.005538 & 0.010435 & 0.018407 & 0.027837 \\
6 & 0.033106 & 0.027837 & 0.018407 & 0.010435 & 0.005538 & 0.003357 & 0.003357 & 0.005538 & 0.010435 & 0.018407 & 0.027837 \\
7 & 0.085283 & 0.067041 & 0.040185 & 0.020607 & 0.009923 & 0.005538 & 0.005538 & 0.009923 & 0.020607 & 0.040185 & 0.067041 \\
8 & 0.280822 & 0.198928 & 0.105160 & 0.047865 & 0.020607 & 0.010435 & 0.010435 & 0.020607 & 0.047865 & 0.105160 & 0.198928 \\
9 & 0.991487 & 0.591645 & 0.265391 & 0.105160 & 0.040185 & 0.018407 & 0.018407 & 0.040185 & 0.105160 & 0.265391 & 0.591645 \\
10 & 3.741537 & 1.650667 & 0.591645 & 0.198928 & 0.067041 & 0.027837 & 0.027837 & 0.067041 & 0.198928 & 0.591645 & \cellcolor{green!40}1.650667 \\

\hline
	\end{tabular}
	}
\end{center}

The agent starts at (10,10) and the prey is located at (0,0). It takes the agent two steps to catch the prey. Using the property that the grid is toroidal minimizes the distance between the agent and the prey. However, it is not possible for the agent to move diagonally in one step.

\begin{center}
	\begin{tabular}{ l | l | l | l | l }
		Predator & Prey & Value & Discount Factor & Iterations to converge \\ 
		\hline
		(0, 0) & (5, 5) & 0.00335 & 0.8 & 33 \\
		(2, 3) & (5, 4) & 0.19892 & 0.8 & 33 \\
		(2, 10) & (10, 0) & 0.19892 & 0.8 & 33 \\
		(10, 10) & (0, 0) & 1.65066 & 0.8 & 33 \\	
	\end{tabular}
\end{center}

The table above proves that policy evaluation to converge always takes equally long for the same size of the grid. This makes sense, as the size of the grid has not changed.

% Temporarily printing in order to compare 

\begin{center}
	\begin{tabular}{ l || l }
		Discount Factor & Iterations to converge \\ 
		\hline
		0.1 & 5 \\
		0.5 & 13 \\
		0.7 & 22 \\
		0.9 & 64 \\	
	\end{tabular}
\end{center}

The discount factor appears to affect the number of iterations necessary to converge. This makes sense as the discount factor discounts the value of a state. A small discount value discounts the value of the state quite radically, leading to quick conversion. However, this quick conversion leaves many states with a random policy. This makes the convergence radical and most likely undesired. Using a higher discount value leads to more iterations before convergence. With a less radical discount, policy evaluation can be optimized in such a way that every state has a value. The discount factor should, however, not be too large. This will lead to faster convergence. Do note that in order to reach convergence, the discount factor must lie between 0-1.

\subsection*{Policy iteration}
Policy iteration is a stationary algorithm to find the optimal policy. This algorithm exists of two steps: policy evaluation and policy iteration. Policy evaluation is demonstrated in the previous section. Policy iteration finds the optimal (deterministic) policy. The table below shows the results for policy iteration with the prey located at (5,5). As this algorithm first performs policy evaluation until convergence and then performs policy improvement, this algorithm is relatively slow and computationally expensive.
% Somehow print the whole grid for Prey(5,5)
\begin{center}
\scalebox{0.6}
	{
		\begin{tabular}{ |l | l | l | l | l | l | l | l | l | l | l | l|}
		\hline
		\multicolumn{11}{|c|}{Policy Iteration Grid  in loop 3, discount 0.8}\\
		\hline
		Indices y\textbackslash x &0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ 

% readability is non-existent, maybe only keep the policy? (NESWH)
\hline
0 & 3.7281  ES &  4.6602  ES &  5.8252  ES &  7.2816  ES &  9.1020  ES &  11.3776  S &  9.1020  WS &  7.2816  WS &  5.8252  WS &  4.6602  WS &  3.7281  WS  \\
1 & 4.6602  ES &  5.8252  ES &  7.2816  ES &  9.1020  ES &  11.3776  ES &  14.2220  S &  11.3776  WS &  9.1020  WS &  7.2816  WS &  5.8252  WS &  4.6602  WS  \\
2 & 5.8252  ES &  7.2816  ES &  9.1020  ES &  11.3776  ES &  14.2220  ES &  17.7776  S &  14.2220  WS &  11.3776  WS &  9.1020  WS &  7.2816  WS &  5.8252  WS  \\
3 & 7.2816  ES &  9.1020  ES &  11.3776  ES &  14.2220  ES &  17.7776  ES &  22.2220  S &  17.7776  WS &  14.2220  WS &  11.3776  WS &  9.1020  WS &  7.2816  WS  \\
4 & 9.1020  ES &  11.3776  ES &  14.2220  ES &  17.7776  ES &  22.2220  ES &  27.7776  S &  22.2220  WS &  17.7776  WS &  14.2220  WS &  11.3776  WS &  9.1020  WS  \\
5 & 11.3776  E &  14.2220  E &  17.7776  E &  22.2220  E &  27.7776  E &  \cellcolor{red!40}22.2220  WENS &  27.7776  W &  22.2220  W &  17.7776  W &  14.2220  W &  11.3776  W  \\
6 & 9.1020  EN &  11.3776  EN &  14.2220  EN &  17.7776  EN &  22.2220  EN &  27.7776  N &  22.2220  WN &  17.7776  WN &  14.2220  WN &  11.3776  WN &  9.1020  WN  \\
7 & 7.2816  EN &  9.1020  EN &  11.3776  EN &  14.2220  EN &  17.7776  EN &  22.2220  N &  17.7776  WN &  14.2220  WN &  11.3776  WN &  9.1020  WN &  7.2816  WN  \\
8 & 5.8252  EN &  7.2816  EN &  9.1020  EN &  11.3776  EN &  14.2220  EN &  17.7776  N &  14.2220  WN &  11.3776  WN &  9.1020  WN &  7.2816  WN &  5.8252  WN  \\
9 & 4.6602  EN &  5.8252  EN &  7.2816  EN &  9.1020  EN &  11.3776  EN &  14.2220  N &  11.3776  WN &  9.1020  WN &  7.2816  WN &  5.8252  WN &  4.6602  WN  \\
10 & 3.7281  EN &  4.6602  EN &  5.8252  EN &  7.2816  EN &  9.1020  EN &  11.3776  N &  9.1020  WN &  7.2816  WN &  5.8252  WN &  4.6602  WN &  3.7281  WN  \\
	\end{tabular}
	}
\end{center}

As described in the introduction, the agent can move North, South, East, West and Wait. To keep notation as clear and concise as possible, only the first letter of the optimal policy is printed. For clarity, the state 'Wait' is renamed to 'Hold' and is depicted as 'H' where applicable. 
Policy iteration shows that a state can make multiple optimal transitions. The optimal transitions all have the same probability of being chosen, while all other transition probabilities are set to zero. This creates an optimal, deterministic policy.

% Is it correct that (5,5) can go to North, East, South, West? 

% Temporarily printing only these examples in order to compare
% Iterative policy evaluation and policy iteration yield exactly the same solution (at least for discount factor = 0.8). 
% Is this expected behavior? 

\begin{center}
	\begin{tabular}{ l | l | l | l | l }
		Predator & Prey & Value & Discount Factor & Iterations to converge \\ 
		\hline
		(0, 0) & (5, 5) & 0.00335 & 0.8 & 2 \\
		(2, 3) & (5, 4) & 0.19892 & 0.8 & 2 \\
		(2, 10) & (10, 0) & 0.19892 & 0.8 & 2 \\
		(10, 10) & (0, 0) & 1.65066 & 0.8 & 2 \\	
	\end{tabular}
\end{center}

% Since it's always 2, maybe it would be interesting to also print the number of iterations it takes for the value grid to 
% converge for both the policy update iteration

\begin{center}
	\begin{tabular}{ l || l }
		Discount Factor & Iterations to converge \\ 
		\hline
		0.1 & 2 \\
		0.5 & 2 \\
		0.7 & 2 \\
		0.9 & 2 \\	
	\end{tabular}
\end{center}


\subsection*{Value iteration}

% Somehow print the whole grid for Prey(5,5)

% Temporarily printing only these examples in order to compare
% Shouldn't the second and third example have the same values like in the previous two methods? 
% Both are 4 states away from the goal
Prey is located at (5, 5)

\begin{center}
\scalebox{0.7}{
\begin{tabular}{ |l | l | l | l | l | l | l | l | l | l | l | l|}
\hline
\multicolumn{11}{|c|}{Value Iteration Grid  in loop 8}\\
\hline
Indices y\textbackslash x &0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ 

\hline
0 & 0.000000 & 0.000000 & 0.000027 & 0.000168 & 0.001049 & 0.006554 & 0.001049 & 0.000168 & 0.000027 & 0.000000 & 0.000000 \\
1 & 0.000000 & 0.000027 & 0.000168 & 0.001049 & 0.006554 & 0.040960 & 0.006554 & 0.001049 & 0.000168 & 0.000027 & 0.000000 \\
2 & 0.000027 & 0.000168 & 0.001049 & 0.006554 & 0.040960 & 0.256000 & 0.040960 & 0.006554 & 0.001049 & 0.000168 & 0.000027 \\
3 & 0.000168 & 0.001049 & 0.006554 & 0.040960 & 0.256000 & 1.600000 & 0.256000 & 0.040960 & 0.006554 & 0.001049 & 0.000168 \\
4 & 0.001049 & 0.006554 & 0.040960 & 0.256000 & 1.600000 & 10.000000 & 1.600000 & 0.256000 & 0.040960 & 0.006554 & 0.001049 \\
5 & 0.006554 & 0.040960 & 0.256000 & 1.600000 & 10.000000 & 0.000000 & 10.000000 & 1.600000 & 0.256000 & 0.040960 & 0.006554 \\
6 & 0.001049 & 0.006554 & 0.040960 & 0.256000 & 1.600000 & 10.000000 & 1.600000 & 0.256000 & 0.040960 & 0.006554 & 0.001049 \\
7 & 0.000168 & 0.001049 & 0.006554 & 0.040960 & 0.256000 & 1.600000 & 0.256000 & 0.040960 & 0.006554 & 0.001049 & 0.000168 \\
8 & 0.000027 & 0.000168 & 0.001049 & 0.006554 & 0.040960 & 0.256000 & 0.040960 & 0.006554 & 0.001049 & 0.000168 & 0.000027 \\
9 & 0.000000 & 0.000027 & 0.000168 & 0.001049 & 0.006554 & 0.040960 & 0.006554 & 0.001049 & 0.000168 & 0.000027 & 0.000000 \\
10 & 0.000000 & 0.000000 & 0.000027 & 0.000168 & 0.001049 & 0.006554 & 0.001049 & 0.000168 & 0.000027 & 0.000000 & 0.000000 \\
\hline
\end{tabular}
}
\end{center}




\begin{center}
	\begin{tabular}{ l || l }
		Discount Factor & Iterations to converge \\ 
		\hline
		0.1 & 1 \\
		0.5 & 7 \\
		0.7 & 7 \\
		0.9 & 8 \\	
	\end{tabular}
\end{center}


\subsection*{Smarter state-space encoding}




\begin{comment}
Sample table in case it's needed
\begin{center}
	\begin{tabular}{ l | l | l | l | l | l || l | l | l | l || l}
		\textbf{K} & Sampling & C-space & VocabSize & TrainSize & Kernel & Airplanes & Cars & Faces & Motorbikes & MAP \\ 
		\hline
		400 & dense & gray & 100 & 300 & linear & 87 & 74 & 95 & 69 & 81 \\
		800 & dense & gray & 100 & 300 & linear & 93 & 70 & 86 & 73 & 81 \\
		1600 & dense & gray & 100 & 300 & linear & 88 & 72 & 86 & 64 & 78 \\
		2000 & dense & gray & 100 & 300 & linear & 86 & 73 & 81 & 51 & 73 \\	
		4000 & dense & gray & 100 & 300 & linear & 90 & 75 & 83 & 53 & 75 \\	
	\end{tabular}
\end{center}

\end{comment}


\section*{Conclusion}

\section*{Files attached}

\section*{Sources}

% We don't have any sources so far, right? :/

\begin{comment}
\begin{itemize}
	\item MatLab (http://www.mathworks.nl/products/matlab/)
	\item VL\_Feat (http://www.vlfeat.org/)
	\item LibSVM (http://www.csie.ntu.edu.tw/cjlin/libsvm/)
	\item Caltech Vision Group (http://www.vision.caltech.edu/)
\end{itemize}
\end{comment}

\end{document}
