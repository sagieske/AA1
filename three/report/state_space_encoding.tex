In the previous assignments, two agents participated in the game. A predator chased a prey, which hardly ever moved and didn't learn. Now, the both the predator and the prey move around on the grid. Both agents learn and it is possible to initialize the game with up to four predators. This leads to a total of five agents. Adding an agent to the game increases the state space exponentially, making computation of Q-learning very expensive. Combined with the fact that all algorithms need time to learn and that multiple experiments must be run for accuracy, the algorithms will be executed several thousand times. It is, therefore, essential to calculate results as quick as possible. This will speed up significantly using state space encoding. Instead of checking the entire grid per agent, the distance to each agent is calculated. Along these distances, the Q-values are evaluated and used to take an action. The Q-values will be updated, not longer needing to update the entire grid after taking an action. Using this technique, only part of the Q-values are updated, per agent. This saves a lot of time, compared to the previous implementation.