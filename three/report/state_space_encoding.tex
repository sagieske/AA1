In the previous assignments, two agents participated in the game. A predator chased a prey, which hardly ever moved and didn't learn. Now, both the predator and the prey move around on the grid. Both agents learn and it is possible to initialize the game with up to four predators, totaling five agents. Adding an agent to the game increases the state space exponentially, making computation of the algorithms very expensive. Combined with the fact that all algorithms need time to learn and that multiple experiments must be run for accuracy, each iteration in each algorithms will be executed several thousand times. It is, therefore, essential to calculate results as quickly as possible. To reduce the state space complexity, it is encoded as follows:  instead of using the locations of all agents as states, the distance to every other agent is calculated. Along these distances, the Q-values are evaluated and used to take an action. The Q-values will be updated for distances between agents, disregarding the absolute locations of agents and thus, taking advantage of the state space symmetry. Using this technique, the number of states to be updated per agents is reduced, resulting in reducing the state space from $11^{2n}$ to $11^{2(n-1)}$ where $n$ is the number of agents in the grid.