\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{xcolor,colortbl}
\usepackage[]{algorithm2e}
%\usepackage{cite}
\usepackage{caption}
\usepackage{mdframed}
\usepackage{float}
%\usepackage{soul}


\usepackage[backend=bibtex]{biblatex}
\addbibresource{bibliography.bib}


\newcommand{\tab}{\hspace{10mm}}
\newcommand{\dtab}{\hspace{20mm}}
\newcommand{\ttab}{\hspace{30mm}}
\newcommand{\qtab}{\hspace{40mm}}

\begin{document}

\title{Autonomous Agents 1 \\ Assignment 3}

\author{By Group 4: Gieske, Gornishka, Koster, Loor}
\maketitle
\pagebreak
\tableofcontents


\pagebreak

\section{Introduction}


\pagebreak

\section{Theory}

\subsection{Independent Q-learning}
In independent Q-learning, all the agents independently use Q-learning to maximize their expected reward.  \\ 

\noindent Q-learning\footnote{More specifically, this is \textit{one-step Q-learning}, which is the algorithm evaluated in this paper.} itself is a temporal difference method that uses the update rule in equation \ref{eq:qupdate}. Because it retrieves the Q-value of the state-action pair where Q(s', a) is maximized, it is an \textit{off-policy} method. The algorithm for Q-learning can be found in pseudocode in figure \ref{alg:qlearning}.

\begin{mdframed}
\begin{align}
Q(s_t, a_t) \leftarrow Q(s_t,a_t) + \alpha \left[ r_{t+1} + \gamma \underset{a}{\text{max}} Q(s_{t+1},a) - Q(s_t,a_t)\right]\label{eq:qupdate}
\end{align}
\end{mdframed}


\begin{center} 
\begin{mdframed}
\begin{algorithm}[H]
Initialize Q(s,a) arbitrarily \\
Repeat (for each episode):\\
\tab Initialize s \\
\tab Repeat (for each step of episode):\\
\dtab Choose a from s' using policy derived from Q (e.g., $\epsilon$-greedy)\\
\dtab Take action a, observe r, s'\\
\dtab Q(s,a) $\leftarrow$ Q(s,a) + $\alpha [ r + \gamma \max_a' Q(s', a') - Q(s, a) ]$  \\
\dtab s $\leftarrow$ s'; \\
\tab until s is terminal\\
\end{algorithm}
\end{mdframed}
\captionof{figure}{The algorithm for one-step Q-learning\cite{bartosutton}}
\label{alg:qlearning}
\end{center}





\subsection{Minmax Q-Learning} 
In minimax Q-learning, each agent uses update rule \ref{ref:minimaxrule}
\begin{mdframed}
\begin{align}
Q(s,a,o) \leftarrow (1-\alpha) Q(s,a,o) + \alpha * (R + \gamma V(s'))
\end{align}
\label{ref:minimaxrule}
\end{mdframed}
Then, linear programming is used to find a policy so that
\begin{mdframed}
\begin{align}
\pi(s,) \leftarrow \underset{\pi'(s,)}{\text{argmax }} \left\{ \underset{o'}{\text{min}} \left\{ \sum_{a'}  \left\{ \pi(s,a') \times Q(s,a',o') \right\} \right\} \right\}
\end{align}
\end{mdframed}



\begin{center} 
\begin{mdframed}
\begin{algorithm}[H]
Initialize Q(s,a) arbitrarily \\
Repeat (for each episode):\\
\tab Initialize s \\
\tab Repeat (for each step of episode):\\
\dtab Choose a from s' using policy derived from Q (e.g., $\epsilon$-greedy)\\
\dtab Take action a, observe reward R, s' and opponent's action o\\
\dtab Q(s,a,o) $\leftarrow$ (1-$\alpha$) Q(s,a,o) + $\alpha$ * (R + $\gamma$ V(s'))  \\
\dtab $\pi(s,) \leftarrow \underset{\pi'(s,)}{\text{argmax }} \left\{ \underset{o'}{\text{min}} \left\{ \sum_{a'}  \left\{ \pi(s,a') \times Q(s,a',o') \right\} \right\} \right\}$ \\
\dtab $ V(s) \leftarrow \underset{o'}{\text{min}} \left\{ \sum_{a'}  \left\{ \pi(s,a') \times Q(s,a',o') \right\} \right\}  $ \\
\dtab $\alpha \leftarrow \alpha * decay$
\end{algorithm}
\end{mdframed}
\captionof{figure}{Minimax-Q learning\cite{Littman94markovgames}}
\label{alg:minmax}
\end{center}





\begin{comment}
Initialize:
For all s in S, a in A, and o in O,
Let Q[s,a,o] := 1
For all s in S,
Let V[s] := 1
For all s in S, a in A,
Let pi[s,a] := 1/|A|
Let alpha := 1.0
Choose an action:
With probability explor, return an action uniformly at random.
Otherwise, if current state is s,
Return action a with probability pi[s,a].
Learn:
After receiving reward rew for moving from state s to s’
via action a and opponent’s action o,
Let Q[s,a,o] := (1-alpha) * Q[s,a,o] + alpha * (rew + gamma * V[s’])
Use linear programming to find pi[s,.] such that:
pi[s,.] := argmaxfpi’[s,.], minfo’, sumfa’, pi[s,a’] * Q[s,a’,o’]ggg
Let V[s] := minfo’, sumfa’, pi[s,a’] * Q[s,a’,o’]gg
Let alpha := alpha * decay
\end{comment}


% Update which one is the `other leanring'
\subsection{Friend-or-Foe}
\input{friend_or_foe}

\section{Implementation}

\subsection{New state space}


\subsection{Files}

The implementation consists of the following files:
\begin{description}
	\item[Agents\_new] \hfill \\ 
	This file contains implementions of the Agent class, the Prey class and the Predator class. Both the predator and the prey inherit functions of the Agent class. The Agent class contains functions any agent needs, such as a set of actions, a policy and other functions. As the predator is the agent is the agent this implementation focuses on, the predator class contains more functions than the predator class.
	
	\item[Helpers] \hfill \\ 
	This file contains many helper functions. These functions aid in computation and decision making, but cannot (and need not) be part of a specific class.
	
	\item[Other\_objects] \hfill \\ % uncertain about policy class description
	This file contains the Policy and Environment classes. The environment of the game as well as the rules are implemented in the Environment class. The Policy class contains the implementation of Q-Learning, Sarsa, $\epsilon$-greedy, softmax action selection and more functions that help in determining and optimizing a policy as well as choosing an action of this policy.
	\item[Newstate] \hfill \\ 
	This file contains the Game class as well as a demonstration function. The Game class instantiates the game, initialized the predator and the prey and assigns policies to these. The game is run N times and the result is printed. The demonstration function also performs Q-Learning, Sarsa and Monte Carlo. It also uses $\epsilon$-greedy and softmax action selection. The results are printed in the command line and graphs are used for analysis.
\end{description}

\pagebreak


\section{Analysis}
This section discusses the results of the implementations. In order to display and compare results, graphs are used. The title describes which parameters are analysed and the legend shows which color represents which setting of said parameters.

\subsection{Independent Q-Learning}

\subsection{Minmax Q-learning}

\subsection{Other Learning}

\pagebreak


\section{Conclusion}

\pagebreak


\section{Future work}

\pagebreak


\section{Files attached}

\begin{itemize}
\item newstate.py
\item agents\_new.py
\item other\_objects.py
\item helpers.py \ldots
\end{itemize}


\section{Sources}

% TODO Update the bibliography according to the new assignment
\nocite{*}
\printbibliography


\begin{comment}
\bibliography{bibliography}
\bibliographystyle{plain}
\begin{itemize}
	\item [1] Barto and Sutton (http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html) \ldots
\end{itemize}
\end{comment}


\end{document}