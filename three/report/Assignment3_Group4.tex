\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{xcolor,colortbl}
\usepackage[]{algorithm2e}
%\usepackage{cite}
\usepackage{caption}
\usepackage{mdframed}
\usepackage{float}
\setcounter{tocdepth}{5}
%\usepackage{soul}

\usepackage[backend=bibtex]{biblatex}
\addbibresource{bibliography.bib}


\newcommand{\tab}{\hspace{10mm}}
\newcommand{\dtab}{\hspace{20mm}}
\newcommand{\ttab}{\hspace{30mm}}
\newcommand{\qtab}{\hspace{40mm}}

\begin{document}

\title{Autonomous Agents 1 \\ Assignment 3}

\author{By Group 4: Gieske, Gornishka, Koster, Loor}
\maketitle

\pagebreak
\tableofcontents


\pagebreak

\section{Introduction}
\input{intro}


\pagebreak

\section{Theory}

\subsection{Independent Q-learning}
In independent Q-learning, all agents independently use Q-learning to maximize their expected reward.  \\ 

\noindent Q-learning\footnote{More specifically, this is \textit{one-step Q-learning}, which is the algorithm evaluated in this paper.} itself is a temporal difference method that uses the update rule in equation \eqref{eq:qupdate}. Since the algorithm retrieves the Q-value of the state-action pair where Q(s', a) is maximized, this is an \textit{off-policy} method. The algorithm for Q-learning can be found in pseudocode in figure \ref{alg:qlearning}.

\begin{mdframed}
\begin{align}
Q(s_t, a_t) \leftarrow Q(s_t,a_t) + \alpha \left[ r_{t+1} + \gamma \underset{a}{\text{max}} Q(s_{t+1},a) - Q(s_t,a_t)\right]\label{eq:qupdate}
\end{align}
\end{mdframed}


\begin{center} 
\begin{mdframed}
\begin{algorithm}[H]
Initialize Q(s,a) arbitrarily \\
Repeat (for each episode):\\
\tab Initialize s \\
\tab Repeat (for each step of episode):\\
\dtab Choose a from s' using policy derived from Q (e.g., $\epsilon$-greedy)\\
\dtab Take action a, observe r, s'\\
\dtab Q(s,a) $\leftarrow$ Q(s,a) + $\alpha [ r + \gamma \max_a' Q(s', a') - Q(s, a) ]$  \\
\dtab s $\leftarrow$ s'; \\
\tab until s is terminal\\
\end{algorithm}
\end{mdframed}
\captionof{figure}{The algorithm for one-step Q-learning\cite{bartosutton}}
\label{alg:qlearning}
\end{center}


\subsection{Independent Sarsa}
\input{sarsa}


\subsection{Minimax Q-Learning} 
\input{minimax_q}

\subsection{Policy Hill Climbing} 
\input{policy_hill_climbing}

\begin{comment}
Initialize:
For all s in S, a in A, and o in O,
Let Q[s,a,o] := 1
For all s in S,
Let V[s] := 1
For all s in S, a in A,
Let pi[s,a] := 1/|A|
Let alpha := 1.0
Choose an action:
With probability explor, return an action uniformly at random.
Otherwise, if current state is s,
Return action a with probability pi[s,a].
Learn:
After receiving reward rew for moving from state s to s’
via action a and opponent’s action o,
Let Q[s,a,o] := (1-alpha) * Q[s,a,o] + alpha * (rew + gamma * V[s’])
Use linear programming to find pi[s,.] such that:
pi[s,.] := argmaxfpi’[s,.], minfo’, sumfa’, pi[s,a’] * Q[s,a’,o’]ggg
Let V[s] := minfo’, sumfa’, pi[s,a’] * Q[s,a’,o’]gg
Let alpha := alpha * decay
\end{comment}


% Update which one is the `other leanring'
%\subsection{Friend-or-Foe}
%\input{friend_or_foe}

\pagebreak

\section{Implementation}

\subsection{New state space}
\input{state_space_encoding}

\subsection{Files}

The implementation consists of the following files:
\begin{description}
	\item[Agents\_new] \hfill \\ 
	This file contains implementions of the Agent class, the Prey class and the Predator class. Both the predator and the prey inherit functions of the Agent class. The Agent class contains functions any agent needs, such as a set of actions, a policy and other functions. As the predator is the agent this implementation focuses on, the predator class contains more functions than the predator class.
	
	\item[Helpers] \hfill \\ 
	This file contains many helper functions. These functions aid in computation and decision making, but cannot (and need not) be part of a specific class.
	
	\item[Other\_objects] \hfill \\ % uncertain about policy class description
	This file contains the Policy and Environment classes. The environment of the game as well as the rules are implemented in the Environment class. The Policy class contains the implementation of Q-Learning, Sarsa and $\epsilon$-greedy action selection and more functions that help in determining and optimizing a policy as well as choosing an action of this policy.
	\item[Newstate] \hfill \\ 
	This file contains the Game class as well as a demonstration function. The Game class instantiates the game, initialized the predator and the prey and assigns policies to these. The game is run N times and the result is printed. The demonstration function also performs independent Q-learning, minimax Q-learning and independent Sarsa. It uses $\epsilon$-greedy action selection. The results are printed in the command line and graphs are used for analysis.
\end{description}

\pagebreak


\section{Analysis}
This section discusses the results of the implementations. In order to display and compare results, graphs are used. The title describes which parameters are analysed and the legend shows which color represents which setting of said parameters. Contrary to previous reports, tests were run 5 times with 2000 episodes. Eventually, the results were averaged and used for analysis. In order to analyse the results, several default parameters were established, based on the previous assignments. These are:

\begin{itemize}
\item 2000 runs
\item 5 experiments
\item 2 predators
\item 0.9 discount factor ($\gamma$)
\item 0.5 learning rate ($\alpha$)
\item 0.1 epsilon ($\epsilon$)
\end{itemize}

When testing the implementation or parameters, only the parameter under test changes. All other parameters have the default value as described above.

\subsection{Independent Q-Learning}
\input{ind_q_analysis}

\subsection{Independent Sarsa}
\input{ind_sarsa_analysis}

\subsection{Minimax Q-learning}
\pagebreak


\section{Conclusion}
\input{conclusion}
\pagebreak


\section{Future work}
\input{future_work}
\pagebreak


\section{Files attached}

\begin{itemize}
\item newstate.py
\item agents\_new.py
\item other\_objects.py
\item helpers.py \ldots
\end{itemize}


\section{Sources}

% TODO Update the bibliography according to the new assignment
\nocite{*}
\printbibliography


\begin{comment}
\bibliography{bibliography}
\bibliographystyle{plain}
\begin{itemize}
	\item [1] Barto and Sutton (http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html) \ldots
\end{itemize}
\end{comment}


\end{document}