Friend-or-Foe Q-learning is a multi-agent reinforcement learning technique in which the agent identifies other agents as a 'friend' or a 'foe' . For 'friend' agents a coordination equilibrium is to be found, that is an equilibrium in which all players achieve their highest possible value. If the other agent is a 'foe' agent an adversarial equilibrium is to be found. This equilibrium has the property that the agent is not hurt by any change of the other agent. The algorithm is an adaption of the Nash-Q update rule as seen in equation \ref{eq:nashq}, where the $NashQ^i$ function for agent $i$ is replaced by a function specified by other agents being friend (\ref{eq:friend}) or foe  (\ref{eq:foe}). \\
\begin{mdframed}
\begin{align}
Q_{t+1}^i(s,a^1, \dots, a^n) = (1-\alpha)Q_t^i(,a^1, \dots, a^n )+ \alpha[r_t^i+\gamma NashQ_t^i(s, Q^1, \dots, Q^n)]\label{eq:nashq}
\end{align}
\end{mdframed}

~\\$NashQ^i$ function if the other agents are friends:\\
\begin{mdframed}
\begin{align}
\max\limits_{a_1\in A_1} Q^i[s,a^1, \dots, a^n]\label{eq:friend}
\end{align}
\end{mdframed}

~\\$NashQ^i$ function if the other agents are foes:\\
\begin{mdframed}
\begin{align}
\max\limits_{\pi \in \prod(A_i)} \min\limits_{a_i \in A_i} Q^i[s,a^1, \dots, a^n]\label{eq:foe}
\end{align}
\end{mdframed}

