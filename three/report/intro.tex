This report contains the analysis of an implementation of multi-agent reinforcement learning. In this case, there can be multiple predators and there is one prey that needs to be caught. The predators all receive a positive reward of 10 if the prey is caught and the prey receives a negative reward of 10. However, if multiple predators bump into one another, they get confused and the prey gets away. In that case, the prey receives a positive reward of 10 and all predators receive a negative reward of 10. 

In this case, all agents learn using model-free learning. Three different ways of model-free learning are used: independent Q-learning, minimax Q-learning and independent SARSA. Though the workings of these methods have been researched extensively in the past, the prey never learned. Now this is different, which makes the game more interesting. Also, it is interesting to see if and how the predators are going to work together in order to catch the prey. Therefore, this paper focuses on the effects of these methods, implemented on all agents. Also, the effects of different parameter settings (learning rate, discount factor and $\epsilon$) are researched.