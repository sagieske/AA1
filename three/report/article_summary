The problem with independent Q-learning is that it is based on a stationary environment, i.e. where the rules stay the same. However, in a Multi-agent environment where other agents learn as well, the environment is dynamic. This means that the guarantees that hold for single-agent learning do not hold in this setting. The article by Littman specifically considers two-player zero-sum games. In this type of Markov Game, it is possible to use a single reward function, that one player tries to \textit{maximize}, and the other (the opponent) tries to \textit{minimize}. For every MDP, there is a policy \pi that is optimal. However, for Markov Games, there is often no \textit{undominated} policy. The solution to this is to pick a policy and compare it to an opponent that takes the worst actions with regards to this policy. In short, minimax picks the policy that maximizes the agent's reward in the worst case. This policy can be stochastic, as seen in the policy for \textit{rock, paper, scissors} (where the best policy is being unpredictable so you cannot be exploited).